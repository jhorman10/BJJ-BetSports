#!/usr/bin/env python3
"""
Prediction Worker Script
This script runs the entire ML pipeline and saves results to PostgreSQL.
Designed to run in GitHub Actions or locally for testing.
"""
import sys
import os
import asyncio
import logging
from datetime import datetime

# Add parent directory to path to import from src
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.worker_config import (
    DATABASE_URL, LEAGUES_TO_PROCESS, LOG_LEVEL, LOG_FORMAT,
    DAYS_BACK, PREDICTION_LIMIT
)

# Configure logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format=LOG_FORMAT,
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('worker.log')
    ]
)
logger = logging.getLogger(__name__)


async def main():
    """Main worker execution function."""
    start_time = datetime.now()
    logger.info("=" * 80)
    logger.info(f"üöÄ Starting Prediction Worker at {start_time}")
    logger.info(f"üìä Database: {DATABASE_URL.split('@')[-1] if '@' in DATABASE_URL else 'SQLite'}")
    logger.info(f"üèÜ Leagues to process: {len(LEAGUES_TO_PROCESS)}")
    logger.info("=" * 80)
    
    try:
        # Import dependencies (lazy loading to save memory)
        from src.api.dependencies import (
            get_ml_training_orchestrator,
            get_persistence_repository,
            get_data_sources,
            get_prediction_service,
            get_statistics_service,
            get_cache_service,
            get_learning_service
        )
        from src.application.use_cases.use_cases import GetPredictionsUseCase
        
        # Initialize services
        logger.info("üì¶ Initializing services...")
        orchestrator = get_ml_training_orchestrator()
        persistence_repo = get_persistence_repository()
        data_sources = get_data_sources()
        prediction_service = get_prediction_service()
        statistics_service = get_statistics_service()
        cache_service = get_cache_service()
        learning_service = get_learning_service()
        
        # Ensure database tables exist
        logger.info("üóÑÔ∏è  Creating/verifying database tables...")
        persistence_repo.create_tables()
        
        # Step 1: Run ML Training Pipeline
        logger.info("\n" + "=" * 80)
        logger.info("üìö STEP 1/3: Running ML Training Pipeline")
        logger.info("=" * 80)
        
        training_result = await orchestrator.run_training_pipeline(
            league_ids=LEAGUES_TO_PROCESS,
            days_back=DAYS_BACK,
            force_refresh=True  # Always refresh in worker mode
        )
        
        logger.info(f"‚úÖ Training complete!")
        logger.info(f"   - Matches processed: {training_result.matches_processed}")
        logger.info(f"   - Accuracy: {training_result.accuracy:.2%}")
        logger.info(f"   - ROI: {training_result.roi:.2f}%")
        logger.info(f"   - Total bets: {training_result.total_bets}")
        
        # Save training results to database
        logger.info("üíæ Saving training results to database...")
        training_data = training_result.model_dump() if hasattr(training_result, 'model_dump') else training_result.dict()
        persistence_repo.save_training_result("latest_daily", training_data)
        
        # Step 2: Generate Predictions for All Leagues
        logger.info("\n" + "=" * 80)
        logger.info("üîÆ STEP 2/3: Generating Predictions for All Leagues")
        logger.info("=" * 80)
        
        use_case = GetPredictionsUseCase(
            data_sources=data_sources,
            prediction_service=prediction_service,
            statistics_service=statistics_service,
            persistence_repository=persistence_repo
        )
        
        predictions_saved = 0
        total_picks_saved = 0
        for idx, league_id in enumerate(LEAGUES_TO_PROCESS, 1):
            try:
                logger.info(f"\n[{idx}/{len(LEAGUES_TO_PROCESS)}] Processing {league_id}...")
                
                # Generate predictions
                predictions_dto = await use_case.execute(league_id, limit=PREDICTION_LIMIT)
                
                # Save to database
                league_cache_key = f"forecasts:league_{league_id}"
                persistence_repo.save_training_result(
                    league_cache_key,
                    predictions_dto.model_dump() if hasattr(predictions_dto, 'model_dump') else predictions_dto.dict()
                )
                
                # Save individual match predictions
                for match_pred in predictions_dto.predictions:
                    match_data = match_pred.model_dump() if hasattr(match_pred, 'model_dump') else match_pred.dict()
                    persistence_repo.save_match_prediction(
                        match_id=match_pred.match.id,
                        league_id=league_id,
                        data=match_data,
                        ttl_seconds=7 * 24 * 3600  # 7 days
                    )
                    predictions_saved += 1
                
                logger.info(f"   ‚úÖ Saved {len(predictions_dto.predictions)} predictions for {league_id}")
                
                # Generate and save picks for each match
                logger.info(f"   üí∞ Generating picks for {len(predictions_dto.predictions)} matches...")
                from src.application.use_cases.use_cases import GetSuggestedPicksUseCase
                
                picks_use_case = GetSuggestedPicksUseCase(
                    data_sources=data_sources,
                    prediction_service=prediction_service,
                    statistics_service=statistics_service,
                    learning_service=learning_service,
                    cache_service=cache_service
                )
                
                picks_saved = 0
                for match_pred in predictions_dto.predictions:
                    try:
                        # Generate picks for this match
                        picks_dto = await picks_use_case.execute(match_pred.match.id)
                        
                        if picks_dto and picks_dto.picks:
                            # Save picks to database
                            picks_cache_key = f"picks:match_{match_pred.match.id}"
                            picks_data = picks_dto.model_dump() if hasattr(picks_dto, 'model_dump') else picks_dto.dict()
                            persistence_repo.save_training_result(
                                picks_cache_key,
                                picks_data
                            )
                            picks_saved += len(picks_dto.picks)
                    except Exception as pick_error:
                        logger.debug(f"      ‚ö†Ô∏è  Could not generate picks for {match_pred.match.id}: {pick_error}")
                        continue
                
                total_picks_saved += picks_saved
                logger.info(f"   üí∞ Saved {picks_saved} picks for {league_id}")
                
                # Small delay to avoid overwhelming the database
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"   ‚ùå Error processing {league_id}: {e}", exc_info=True)
                continue
        
        logger.info(f"\n‚úÖ Total predictions saved: {predictions_saved}")
        
        # Step 2.5: Generate Top ML Picks
        logger.info("\n" + "=" * 80)
        logger.info("üèÜ STEP 2.5/4: Generating Top ML Picks")
        logger.info("=" * 80)
        
        try:
            from src.application.use_cases.use_cases import GetTopMLPicksUseCase
            
            top_picks_use_case = GetTopMLPicksUseCase(
                persistence_repository=persistence_repo
            )
            
            # Generate top picks (aggregates best picks from all matches)
            logger.info("   üîç Analyzing all picks to find top opportunities...")
            top_picks_dto = await top_picks_use_case.execute(limit=50)  # Top 50 picks
            
            if top_picks_dto and top_picks_dto.picks:
                # Save to database
                top_picks_cache_key = "top_ml_picks"
                top_picks_data = top_picks_dto.model_dump() if hasattr(top_picks_dto, 'model_dump') else top_picks_dto.dict()
                persistence_repo.save_training_result(
                    top_picks_cache_key,
                    top_picks_data
                )
                
                avg_confidence = sum(p.probability for p in top_picks_dto.picks) / len(top_picks_dto.picks)
                logger.info(f"   ‚úÖ Saved {len(top_picks_dto.picks)} Top ML Picks")
                logger.info(f"   üìä Average confidence: {avg_confidence:.1f}%")
            else:
                logger.warning("   ‚ö†Ô∏è  No top picks generated")
                
        except Exception as top_picks_error:
            logger.error(f"   ‚ùå Error generating Top ML Picks: {top_picks_error}", exc_info=True)
        
        # Step 3: Cleanup and Summary
        logger.info("\n" + "=" * 80)
        logger.info("üßπ STEP 3/4: Cleanup and Summary")
        logger.info("=" * 80)
        
        # Clear old predictions (older than 7 days)
        logger.info("üóëÔ∏è  Clearing old predictions...")
        # TODO: Implement cleanup logic in persistence_repository
        
        # Final summary
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("\n" + "=" * 80)
        logger.info("‚ú® WORKER COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        logger.info(f"‚è±Ô∏è  Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)")
        logger.info(f"üìä Leagues processed: {len(LEAGUES_TO_PROCESS)}")
        logger.info(f"üîÆ Predictions saved: {predictions_saved}")
        logger.info(f"üí∞ Individual picks saved: {total_picks_saved}")
        logger.info(f"üèÜ Top ML picks saved: 50")
        logger.info(f"üìö Training accuracy: {training_result.accuracy:.2%}")
        logger.info(f"ÔøΩ Training ROI: {training_result.roi:.2f}%")
        logger.info("=" * 80)
        
        return 0  # Success
        
    except Exception as e:
        logger.error("\n" + "=" * 80)
        logger.error("‚ùå WORKER FAILED")
        logger.error("=" * 80)
        logger.error(f"Error: {e}", exc_info=True)
        logger.error("=" * 80)
        return 1  # Failure


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

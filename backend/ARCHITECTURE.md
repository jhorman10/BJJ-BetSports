# ğŸ—ï¸ Serverless Architecture - Worker & API Separation

## Overview

This project now uses a **serverless architecture** that separates heavy ML computations from the lightweight API server:

- **GitHub Actions (The Brain)**: Runs ML training and prediction generation every 6 hours
- **PostgreSQL (The Memory)**: Stores all pre-computed predictions and training results
- **FastAPI on Render (The Face)**: Lightweight API that only reads from the database

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Actions Worker             â”‚
â”‚   (Every 6 hours)                   â”‚
â”‚                                     â”‚
â”‚   1. Fetch historical data          â”‚
â”‚   2. Train ML models                â”‚
â”‚   3. Generate predictions           â”‚
â”‚   4. Save to PostgreSQL             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL Database               â”‚
â”‚   - Training results                â”‚
â”‚   - League predictions              â”‚
â”‚   - Match predictions               â”‚
â”‚   - Picks & statistics              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Server (Render)           â”‚
â”‚   API_ONLY_MODE=true                â”‚
â”‚                                     â”‚
â”‚   - No ML computations              â”‚
â”‚   - Only database reads             â”‚
â”‚   - Memory: ~100-150MB              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Local Development (Full Mode)

```bash
# Install all dependencies (including ML libraries)
cd backend
pip install -r requirements-worker.txt

# Run the server in full mode (with ML computations)
API_ONLY_MODE=false uvicorn src.api.main:app --reload

# Or run the worker script manually
python scripts/run_predictions.py
```

### Production (API-Only Mode)

```bash
# Install lightweight dependencies only
pip install -r requirements.txt

# Run the server in API-only mode
API_ONLY_MODE=true uvicorn src.api.main:app
```

## ğŸ“ File Structure

```
backend/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_predictions.py      # Main worker script
â”‚   â””â”€â”€ worker_config.py        # Worker configuration
â”œâ”€â”€ requirements.txt            # Lightweight API dependencies (~50MB)
â”œâ”€â”€ requirements-worker.txt     # Full ML dependencies (~400MB)
â””â”€â”€ .env.example               # Environment variables template

.github/
â””â”€â”€ workflows/
    â””â”€â”€ update_predictions.yml  # GitHub Actions workflow
```

## ğŸ”§ Configuration

### Environment Variables

#### Required for Production

```bash
# Architecture mode
API_ONLY_MODE=true              # Enable lightweight API mode

# Database (REQUIRED in production)
DATABASE_URL=postgresql://user:pass@host:port/db

# API Keys (for worker)
FOOTBALL_DATA_ORG_KEY=your_key
THE_ODDS_API_KEY=your_key
```

#### Optional

```bash
# Development
API_ONLY_MODE=false             # Enable full ML mode locally
LOW_MEMORY_MODE=false           # Disable for local development
DISABLE_ML_TRAINING=false       # Enable training locally
CLEAR_CACHE_ON_START=false      # Clear cache on startup

# Logging
LOG_LEVEL=INFO
```

### GitHub Actions Secrets

Add these secrets to your GitHub repository:

1. Go to **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Add the following secrets:
   - `DATABASE_URL`: PostgreSQL connection string
   - `FOOTBALL_DATA_ORG_KEY`: Football-Data.org API key
   - `THE_ODDS_API_KEY`: The Odds API key
   - `RAPIDAPI_KEY`: RapidAPI key (optional)

## ğŸ”„ Worker Script

### Manual Execution

```bash
cd backend
python scripts/run_predictions.py
```

### What It Does

1. **Training Phase** (~5-10 minutes)

   - Fetches 4,912 historical matches from multiple sources
   - Trains Random Forest classifier
   - Calculates global statistical averages
   - Saves training results to database

2. **Prediction Phase** (~10-15 minutes)

   - Generates predictions for all leagues
   - Calculates suggested picks for each match
   - Saves individual match predictions to database
   - Total: ~500-1000 predictions saved

3. **Cleanup**
   - Removes old predictions (>7 days)
   - Logs summary statistics

### Output

```
================================================================================
âœ¨ WORKER COMPLETED SUCCESSFULLY
================================================================================
â±ï¸  Duration: 847.23 seconds (14.12 minutes)
ğŸ“Š Leagues processed: 16
ğŸ”® Predictions saved: 847
ğŸ“š Training accuracy: 67.45%
ğŸ’° Training ROI: 12.34%
================================================================================
```

## ğŸ¤– GitHub Actions Workflow

### Schedule

The worker runs automatically:

- **Every 6 hours**: `0 */6 * * *` (00:00, 06:00, 12:00, 18:00 UTC)
- **Manual trigger**: Via GitHub Actions UI

### Manual Trigger

1. Go to **Actions** tab in GitHub
2. Select **Update Predictions** workflow
3. Click **Run workflow**
4. Wait ~15-20 minutes for completion

### Monitoring

- View logs in **Actions** tab
- Download worker logs as artifacts (retained for 7 days)
- Automatic issue creation on failure

## ğŸ“Š Memory Usage Comparison

| Mode          | Dependencies            | RAM Usage  | Use Case                   |
| ------------- | ----------------------- | ---------- | -------------------------- |
| **API-Only**  | requirements.txt        | ~100-150MB | Production (Render)        |
| **Full Mode** | requirements-worker.txt | ~512MB+    | Local dev / GitHub Actions |

### Dependencies Removed in API-Only Mode

- âŒ `pandas` (~100MB)
- âŒ `numpy` (~50MB)
- âŒ `scikit-learn` (~100MB)
- âŒ `scipy` (~50MB)
- âŒ `joblib` (~10MB)
- âŒ `apscheduler` (~5MB)

**Total savings: ~300-400MB**

## ğŸ§ª Testing

### Test Worker Locally

```bash
# 1. Set up test database
export DATABASE_URL="postgresql://localhost/bjj_test"

# 2. Run worker
python scripts/run_predictions.py

# 3. Check logs
tail -f worker.log
```

### Test API-Only Mode

```bash
# 1. Start server in API-only mode
API_ONLY_MODE=true uvicorn src.api.main:app

# 2. Check startup logs (should skip ML initialization)
# Expected: "ğŸš€ Starting in API-ONLY MODE (Lightweight)"

# 3. Test endpoint
curl http://localhost:8000/api/v1/predictions/league/E0

# 4. If no data: empty predictions array
# After worker runs: full predictions
```

## ğŸš¢ Deployment

### Render Configuration

1. **Environment Variables**:

   ```
   API_ONLY_MODE=true
   DATABASE_URL=<your-postgres-url>
   ```

2. **Build Command**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Start Command**:
   ```bash
   uvicorn src.api.main:app --host 0.0.0.0 --port $PORT
   ```

### Expected Behavior

1. **First Deploy** (before worker runs):

   - Server starts successfully
   - API returns empty predictions
   - Memory usage: ~100-150MB

2. **After Worker Runs**:
   - API serves predictions from database
   - No ML computations on server
   - Memory usage stays low

## ğŸ› Troubleshooting

### "No predictions available"

**Cause**: Worker hasn't run yet or database is empty

**Solution**:

```bash
# Manually trigger GitHub Actions workflow
# OR run worker locally:
python scripts/run_predictions.py
```

### "API_ONLY_MODE but still high memory"

**Cause**: Wrong requirements file installed

**Solution**:

```bash
# Reinstall with correct requirements
pip uninstall pandas numpy scikit-learn scipy joblib
pip install -r requirements.txt
```

### "Worker fails in GitHub Actions"

**Cause**: Missing secrets or database connection

**Solution**:

1. Check GitHub secrets are set correctly
2. Verify DATABASE_URL is accessible from GitHub Actions
3. Check workflow logs for specific error

## ğŸ“š Additional Resources

- [Implementation Plan](/.gemini/antigravity/brain/b9e031cf-d2c8-405f-b10d-b2f432b72199/implementation_plan.md)
- [Task Breakdown](/.gemini/antigravity/brain/b9e031cf-d2c8-405f-b10d-b2f432b72199/task.md)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Render Documentation](https://render.com/docs)

## ğŸ¯ Benefits

âœ… **Reduced Memory Usage**: 512MB â†’ 100-150MB (70% reduction)
âœ… **Faster Deployments**: No ML model loading on startup
âœ… **Better Reliability**: Pre-computed predictions always available
âœ… **Cost Savings**: Can use smaller Render instance
âœ… **Scalability**: Worker can run on powerful GitHub Actions runners
âœ… **Separation of Concerns**: API and compute are independent
